{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pdb\n",
    "\n",
    "torch.set_printoptions(linewidth = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        #(1) input layer\n",
    "        t = t\n",
    "        \n",
    "        #(2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #(3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #(4) hidden linear layer\n",
    "        t = t.reshape(-1,12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "        \n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=100\n",
    ")\n",
    "optimizer = optim.Adam(network.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 47988 loss: 319.5780900269747\n",
      "epoch: 1 total_correct: 51861 loss: 219.90778167545795\n",
      "epoch: 2 total_correct: 52371 loss: 204.616062566638\n",
      "epoch: 3 total_correct: 52649 loss: 197.41518412530422\n",
      "epoch: 4 total_correct: 53012 loss: 188.01862213015556\n",
      "epoch: 5 total_correct: 53177 loss: 183.28284583985806\n",
      "epoch: 6 total_correct: 53258 loss: 181.4121125638485\n",
      "epoch: 7 total_correct: 53294 loss: 181.53220336139202\n",
      "epoch: 8 total_correct: 53400 loss: 178.20476151257753\n",
      "epoch: 9 total_correct: 53453 loss: 178.9490063637495\n",
      "epoch: 10 total_correct: 53633 loss: 170.74077335000038\n",
      "epoch: 11 total_correct: 53715 loss: 170.99706882983446\n",
      "epoch: 12 total_correct: 53805 loss: 169.94547184556723\n",
      "epoch: 13 total_correct: 53877 loss: 167.18902680277824\n",
      "epoch: 14 total_correct: 53882 loss: 167.71564829349518\n",
      "epoch: 15 total_correct: 53996 loss: 163.1985973417759\n",
      "epoch: 16 total_correct: 53948 loss: 165.8886448070407\n",
      "epoch: 17 total_correct: 53988 loss: 165.68597032874823\n",
      "epoch: 18 total_correct: 54082 loss: 165.3801923841238\n",
      "epoch: 19 total_correct: 54003 loss: 165.5236760750413\n",
      "epoch: 20 total_correct: 54197 loss: 159.48642683774233\n",
      "epoch: 21 total_correct: 54226 loss: 158.50716093182564\n",
      "epoch: 22 total_correct: 54328 loss: 157.80832723528147\n",
      "epoch: 23 total_correct: 54268 loss: 158.8112227767706\n",
      "epoch: 24 total_correct: 54289 loss: 159.60167087614536\n",
      "epoch: 25 total_correct: 54440 loss: 156.27909548580647\n",
      "epoch: 26 total_correct: 54471 loss: 154.06072102487087\n",
      "epoch: 27 total_correct: 54099 loss: 170.2252893447876\n",
      "epoch: 28 total_correct: 54137 loss: 166.57592222839594\n",
      "epoch: 29 total_correct: 54636 loss: 148.05755445361137\n",
      "epoch: 30 total_correct: 54537 loss: 150.45066591352224\n",
      "epoch: 31 total_correct: 54518 loss: 152.65394785255194\n",
      "epoch: 32 total_correct: 54328 loss: 160.4492326080799\n",
      "epoch: 33 total_correct: 54443 loss: 157.95257848501205\n",
      "epoch: 34 total_correct: 54397 loss: 156.8108332902193\n",
      "epoch: 35 total_correct: 54241 loss: 161.94096683710814\n",
      "epoch: 36 total_correct: 54522 loss: 154.09627176821232\n",
      "epoch: 37 total_correct: 54484 loss: 154.77216616645455\n",
      "epoch: 38 total_correct: 54616 loss: 151.4100144058466\n",
      "epoch: 39 total_correct: 54457 loss: 156.35538215190172\n",
      "epoch: 40 total_correct: 54556 loss: 153.41985630244017\n",
      "epoch: 41 total_correct: 54780 loss: 146.52002830058336\n",
      "epoch: 42 total_correct: 54959 loss: 140.28832598775625\n",
      "epoch: 43 total_correct: 54769 loss: 147.82971497625113\n",
      "epoch: 44 total_correct: 54533 loss: 155.5816774070263\n",
      "epoch: 45 total_correct: 54723 loss: 149.0821330025792\n",
      "epoch: 46 total_correct: 54489 loss: 156.85044933110476\n",
      "epoch: 47 total_correct: 54803 loss: 146.47547582536936\n",
      "epoch: 48 total_correct: 54865 loss: 147.56834586709738\n",
      "epoch: 49 total_correct: 54433 loss: 162.63638631999493\n"
     ]
    }
   ],
   "source": [
    "images,labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images,labels = batch\n",
    "\n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "        \n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "    \n",
    "    tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "    tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "    tb.add_histogram(\n",
    "        'conv1.weight.grad'\n",
    "        ,network.conv1.weight.grad\n",
    "        ,epoch\n",
    "    )\n",
    "\n",
    "    print('epoch:',epoch,\n",
    "          'total_correct:', total_correct,\n",
    "          'loss:', total_loss)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
