{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pdb\n",
    "\n",
    "torch.set_printoptions(linewidth = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self,t):\n",
    "        #(1) input layer\n",
    "        t = t\n",
    "        \n",
    "        #(2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #(3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        #(4) hidden linear layer\n",
    "        t = t.reshape(-1,12 * 4 * 4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1)\n",
    "        \n",
    "        return t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=100\n",
    "# )\n",
    "# optimizer = optim.Adam(network.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [100,1000,10000]\n",
    "lr_list = [.01,.001,.0001,.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 47585 loss: 33033.57497751713\n",
      "epoch: 1 total_correct: 51485 loss: 23053.107208013535\n",
      "epoch: 2 total_correct: 52071 loss: 21369.594030082226\n",
      "epoch: 3 total_correct: 52386 loss: 20374.2847725749\n",
      "epoch: 4 total_correct: 52562 loss: 19908.495746552944\n",
      "epoch: 0 total_correct: 42015 loss: 46570.40399312973\n",
      "epoch: 1 total_correct: 48276 loss: 30957.88463652134\n",
      "epoch: 2 total_correct: 50276 loss: 26423.177137970924\n",
      "epoch: 3 total_correct: 51535 loss: 23399.070486426353\n",
      "epoch: 4 total_correct: 52146 loss: 21560.758033394814\n",
      "epoch: 0 total_correct: 32649 loss: 82395.9949016571\n",
      "epoch: 1 total_correct: 42713 loss: 46788.60059380531\n",
      "epoch: 2 total_correct: 44248 loss: 41884.788912534714\n",
      "epoch: 3 total_correct: 45226 loss: 39024.42030310631\n",
      "epoch: 4 total_correct: 45810 loss: 37152.90271937847\n",
      "epoch: 0 total_correct: 11309 loss: 136967.87161827087\n",
      "epoch: 1 total_correct: 23829 loss: 126858.8387966156\n",
      "epoch: 2 total_correct: 30253 loss: 101814.82248306274\n",
      "epoch: 3 total_correct: 34424 loss: 81641.00608825684\n",
      "epoch: 4 total_correct: 36643 loss: 71690.40297269821\n",
      "epoch: 0 total_correct: 35288 loss: 64237.24114894867\n",
      "epoch: 1 total_correct: 46696 loss: 34314.69199061394\n",
      "epoch: 2 total_correct: 49763 loss: 27930.60576915741\n",
      "epoch: 3 total_correct: 51239 loss: 24022.86183834076\n",
      "epoch: 4 total_correct: 52120 loss: 21658.364951610565\n",
      "epoch: 0 total_correct: 26122 loss: 99425.65304040909\n",
      "epoch: 1 total_correct: 42747 loss: 46479.94267940521\n",
      "epoch: 2 total_correct: 44936 loss: 39903.24026346207\n",
      "epoch: 3 total_correct: 45877 loss: 36879.0265917778\n",
      "epoch: 4 total_correct: 46646 loss: 34729.29006814957\n",
      "epoch: 0 total_correct: 11506 loss: 137449.8426914215\n",
      "epoch: 1 total_correct: 18953 loss: 130376.48487091064\n",
      "epoch: 2 total_correct: 26226 loss: 103574.49293136597\n",
      "epoch: 3 total_correct: 36779 loss: 72863.51823806763\n",
      "epoch: 4 total_correct: 39664 loss: 58857.42223262787\n",
      "epoch: 0 total_correct: 6002 loss: 138336.08508110046\n",
      "epoch: 1 total_correct: 6004 loss: 138157.5312614441\n",
      "epoch: 2 total_correct: 6010 loss: 137948.4360218048\n",
      "epoch: 3 total_correct: 6065 loss: 137689.36681747437\n",
      "epoch: 4 total_correct: 6216 loss: 137356.79841041565\n",
      "epoch: 0 total_correct: 14251 loss: 120525.05493164062\n",
      "epoch: 1 total_correct: 24358 loss: 84898.45275878906\n",
      "epoch: 2 total_correct: 33322 loss: 65712.64386177063\n",
      "epoch: 3 total_correct: 38647 loss: 55337.27049827576\n",
      "epoch: 4 total_correct: 41477 loss: 49229.586124420166\n",
      "epoch: 0 total_correct: 9019 loss: 137610.70013046265\n",
      "epoch: 1 total_correct: 20620 loss: 134601.15432739258\n",
      "epoch: 2 total_correct: 25487 loss: 126336.06195449829\n",
      "epoch: 3 total_correct: 29615 loss: 108261.34443283081\n",
      "epoch: 4 total_correct: 33009 loss: 85193.86649131775\n",
      "epoch: 0 total_correct: 6710 loss: 138279.9768447876\n",
      "epoch: 1 total_correct: 8500 loss: 138191.48778915405\n",
      "epoch: 2 total_correct: 9865 loss: 138103.51371765137\n",
      "epoch: 3 total_correct: 10646 loss: 138003.08465957642\n",
      "epoch: 4 total_correct: 11087 loss: 137884.84573364258\n",
      "epoch: 0 total_correct: 6000 loss: 138503.75652313232\n",
      "epoch: 1 total_correct: 6000 loss: 138481.18782043457\n",
      "epoch: 2 total_correct: 6000 loss: 138458.24718475342\n",
      "epoch: 3 total_correct: 6000 loss: 138435.2469444275\n",
      "epoch: 4 total_correct: 6000 loss: 138412.0750427246\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "        network = Network()\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_set, batch_size=batch_size\n",
    "        )\n",
    "        optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "        \n",
    "        images,labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment=f' batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images', grid)\n",
    "        tb.add_graph(network, images)\n",
    "\n",
    "        for epoch in range(5):\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "\n",
    "            for batch in train_loader:\n",
    "                images,labels = batch #get batch\n",
    "\n",
    "                preds = network(images) #pass batch\n",
    "                loss = F.cross_entropy(preds,labels) # calculate loss\n",
    "\n",
    "                optimizer.zero_grad() #zero gradients\n",
    "                loss.backward() # calculate gradients\n",
    "                optimizer.step()# Update weights\n",
    "\n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_correct += get_num_correct(preds,labels)\n",
    "\n",
    "            tb.add_scalar('Loss', total_loss, epoch)\n",
    "            tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "            tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "            \n",
    "            for name, param in network.named_parameters():\n",
    "                tb.add_histogram(name, param, epoch)\n",
    "                tb.add_histogram(f'{name}.grad',param.grad,epoch)\n",
    "                \n",
    "            for name, weight in network.named_parameters():\n",
    "                tb.add_histogram(name, weight, epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight.grad,epoch)\n",
    "            \n",
    "            print('epoch:',epoch,\n",
    "                  'total_correct:', total_correct,\n",
    "                  'loss:', total_loss)\n",
    "        tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        \n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\n",
    "        \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_loader = torch.utils.data.DataLoader(train_set, batch_size =10000)\n",
    "train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_preds.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CatBackward at 0x1a37b06ef0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction_loader = torch.utils.data.DataLoader(train_set, batch_size =100)\n",
    "    train_preds = get_all_preds(network, prediction_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(train_preds.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total correct: 6000\n",
      "accuracy: 0.1\n"
     ]
    }
   ],
   "source": [
    "preds_correct = get_num_correct(train_preds, train_set.targets)\n",
    "\n",
    "print('total correct:', preds_correct)\n",
    "print('accuracy:', preds_correct/len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0,  ..., 3, 0, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 7,  ..., 7, 7, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = torch.stack((train_set.targets,train_preds.argmax(dim=1)),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9, 7],\n",
       "        [0, 7],\n",
       "        [0, 7],\n",
       "        ...,\n",
       "        [3, 7],\n",
       "        [0, 7],\n",
       "        [5, 7]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = torch.zeros(10,10,dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in stacked:\n",
    "    tl,pl = p.tolist()\n",
    "    cmt[tl,pl] = cmt[tl,pl] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0],\n",
       "        [    0,     0,     0,     0,     0,     0,     0, 12000,     0,     0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'resources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-e138a6e66773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotcm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'resources'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from resources.plotcm import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,    0, 6000,    0,    0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamExtractor():\n",
    "    \"\"\"\n",
    "        Extracts cam features from the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward_pass_on_convolutions(self, x):\n",
    "        \"\"\"\n",
    "            Does a forward pass on convolutions, hooks the function at given layer\n",
    "        \"\"\"\n",
    "        conv_output = None\n",
    "        for module_name, module in self.model._modules.items():\n",
    "            print(module_name)\n",
    "            if module_name == 'fc':\n",
    "                return conv_output, x\n",
    "            x = module(x)  # Forward\n",
    "            #print(module_name, module)\n",
    "            if module_name == self.target_layer:\n",
    "                print('True')\n",
    "                x.register_hook(self.save_gradient)\n",
    "                conv_output = x  # Save the convolution output on that layer\n",
    "        return conv_output, x\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        \"\"\"\n",
    "            Does a full forward pass on the model\n",
    "        \"\"\"\n",
    "        # Forward pass on the convolutions\n",
    "        conv_output, x = self.forward_pass_on_convolutions(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        # Forward pass on the classifier\n",
    "        x = self.model.fc(x)\n",
    "        return conv_output, x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
